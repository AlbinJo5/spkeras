{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spkeras.models import cnn_to_snn\n",
    "from spkeras.utils import save_pickle, load_pickle\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "with tf.device(gpu):\n",
    "    cnn_mdl = load_model('../../cnn_model/cnn_mdl.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalute thresholding over different timesteps\n",
    "timesteps = [16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256]\n",
    "\n",
    "thresholding = [0,0.5]\n",
    "scaling_factor = [1]\n",
    "\n",
    "cnn_result = []\n",
    "snn_result = []\n",
    "\n",
    "_, cnn_acc = cnn_mdl.evaluate(x_test,y_test)\n",
    "snn_mdl = cnn_to_snn()(cnn_mdl,x_train)\n",
    "\n",
    "result = []\n",
    "for s1 in thresholding:\n",
    "    _result = []\n",
    "    for s2 in scaling_factor:\n",
    "        __result = []\n",
    "        for t in timesteps:\n",
    "            _,acc = snn_mdl.evaluate(x_test,y_test,timesteps=t,\n",
    "                                        thresholding=s1,                                        \n",
    "                                        scaling_factor=s2)\n",
    "            __result.append(acc)\n",
    "        _result.append(__result)\n",
    "        \n",
    "    result.append(_result)\n",
    "    \n",
    "cnn_result.append(cnn_acc)\n",
    "snn_result.append(result)   \n",
    "             \n",
    "    \n",
    "save_pickle(snn_result,'mdl'+'_snn_result','../results/')\n",
    "save_pickle(cnn_result,'mdl'+'_cnn_result','../results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalute bitwidth over different timesteps\n",
    "timesteps = [16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256]\n",
    "thresholding = [0.5]\n",
    "scaling_factor = [1]\n",
    "bit = [7,8,9,10]\n",
    "\n",
    "cnn_result = []\n",
    "snn_result = []\n",
    "\n",
    "_, cnn_acc = cnn_mdl.evaluate(x_test,y_test)\n",
    "cnn_result.append(cnn_acc)\n",
    "result_=[]\n",
    "for b in bit:\n",
    "    snn_mdl = cnn_to_snn(signed_bit=b)(cnn_mdl,x_train)\n",
    "    \n",
    "    result = []\n",
    "    for s1 in thresholding:\n",
    "        _result = []\n",
    "        for s2 in scaling_factor:\n",
    "            __result = []\n",
    "            for t in timesteps:\n",
    "                _,acc = snn_mdl.evaluate(x_test,y_test,timesteps=t,\n",
    "                                            thresholding=s1,                                        \n",
    "                                            scaling_factor=s2,\n",
    "                                            spike_ext=0)\n",
    "                __result.append(acc)\n",
    "            _result.append(__result)\n",
    "            \n",
    "        result.append(_result)\n",
    "    result_.append(result)\n",
    "        \n",
    "snn_result.append(result_)   \n",
    "             \n",
    "    \n",
    "save_pickle(snn_result,'mdl_bit'+'_snn_result','../results/')\n",
    "save_pickle(cnn_result,'mdl_bit'+'_cnn_result','../results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract MOps over different timesteps\n",
    "timesteps = [16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256]\n",
    "\n",
    "thresholding = [0,0.5]\n",
    "scaling_factor = [1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "cnn_result = []\n",
    "snn_result = []\n",
    "snn_result_ = []\n",
    "\n",
    "_, cnn_acc = cnn_mdl.evaluate(x_test,y_test)\n",
    "snn_mdl = cnn_to_snn()(cnn_mdl,x_train)\n",
    "\n",
    "result = []\n",
    "result_ = []\n",
    "shape,Neuros = snn_mdl.NeuronNumbers(mode=1) #extract connections\n",
    "\n",
    "for s1 in thresholding:\n",
    "    _result = []\n",
    "    _result_ = []\n",
    "    for s2 in scaling_factor:\n",
    "        __result = []\n",
    "        __result_ = []\n",
    "        for t in timesteps:\n",
    "            #extract spikes\n",
    "            l,N = snn_mdl.SpikeCounter(x_train,timesteps=t,\n",
    "                                        thresholding=s1,                                        \n",
    "                                        scaling_factor=s2)\n",
    "            #Count operations \n",
    "            inp_spikes = x_train*t\n",
    "            inp_spikes = np.floor(inp_spikes)\n",
    "            inp_spikes = (np.sum(inp_spikes)/np.prod(x_train.shape))*np.prod(shape[0])\n",
    "            inp_spikes = np.ceil(inp_spikes)\n",
    "            \n",
    "            a = []\n",
    "            for k in range(len(Neuros)):\n",
    "                if k == len(Neuros) - 1:\n",
    "                    continue\n",
    "                _a = N[k]/Neuros[k]*np.prod(shape[k+1])\n",
    "                a.append(_a)\n",
    "                \n",
    "            __result_.append(np.max(l))\n",
    "            __result.append(np.floor(np.sum(a+inp_spikes)))\n",
    "        _result.append(__result)\n",
    "        _result_.append(__result_)\n",
    "    result.append(_result)\n",
    "    result_.append(_result_)\n",
    "    \n",
    "snn_result.append(result) \n",
    "snn_result_.append(result)\n",
    "             \n",
    "save_pickle(snn_result,'mdl_spikes'+'_snn_result','../results/')\n",
    "save_pickle(snn_result_,'mdl_max_train'+'_snn_result','../results/')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count neurons under different spike numbers at 64 timesteps (None loss mode)\n",
    "timesteps = [64]\n",
    "thresholding = [0]\n",
    "scaling_factor = [1]\n",
    "\n",
    "cnn_result = []\n",
    "snn_result = []\n",
    "\n",
    "_, cnn_acc = cnn_mdl.evaluate(x_test,y_test)\n",
    "snn_mdl = cnn_to_snn()(cnn_mdl,x_train)\n",
    "\n",
    "result = []\n",
    "for s1 in thresholding:\n",
    "    _result = []\n",
    "    for s2 in scaling_factor:\n",
    "        __result = []\n",
    "        for t in timesteps:\n",
    "            _,N = snn_mdl.SpikeCounter(x_train,timesteps=t,\n",
    "                                       thresholding=s1,                                        \n",
    "                                       scaling_factor=s2,\n",
    "                                       noneloss=True,\n",
    "                                       mode=1)\n",
    "            \n",
    "            __result.append(N)\n",
    "            \n",
    "        _result.append(__result)\n",
    "        \n",
    "    result.append(_result)\n",
    "\n",
    "snn_result.append(result) \n",
    "             \n",
    "save_pickle(snn_result,'mdl_spikes_count64_noneloss'+'_snn_result','../results/')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
